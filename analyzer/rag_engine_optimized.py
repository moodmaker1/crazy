"""
rag_engine.py
-------------
Gemini-2.5-Flash + FAISS ê¸°ë°˜ RAG ì—”ì§„ (max_output_tokens í…ŒìŠ¤íŠ¸ ë²„ì „)
- í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ìœ ì§€
- MPS + FP16
- Timeout ì—†ìŒ
- Context ì••ì¶• + ìºì‹œ ìœ ì§€
- ì¶œë ¥ ê¸¸ì´ ì œí•œ (max_output_tokens=500)
"""

import os
import json
import time
import traceback
import hashlib
import numpy as np
import faiss
from typing import Dict, Any
import google.generativeai as genai
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer

# ------------------------------------------------
# âœ… ê¸°ë³¸ ì„¤ì •
# ------------------------------------------------
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

load_dotenv()
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
print("ğŸ” GEMINI_API_KEY =", "âœ… ë¡œë“œ ì™„ë£Œ" if os.getenv("GEMINI_API_KEY") else "âŒ ì—†ìŒ")

# ------------------------------------------------
# âœ… ì„ë² ë”© ëª¨ë¸ (ì „ì—­ 1íšŒ ë¡œë“œ)
# ------------------------------------------------
print("ğŸš€ [Init] ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹œì‘...")
t0 = time.time()
try:
    embedder = SentenceTransformer("BAAI/bge-m3", device="mps")
    if hasattr(embedder, "half"):
        embedder = embedder.half()
    print(f"âœ… [Init] ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({time.time() - t0:.2f}s, device=MPS)")
except Exception as e:
    embedder = None
    print("âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨:", e)

# ------------------------------------------------
# âœ… ìºì‹œ ì´ˆê¸°í™”
# ------------------------------------------------
_cache = {}

def get_cache_key(mct_id: str, mode: str, combined_context: str) -> str:
    return f"{mct_id}-{mode}-{hashlib.md5(combined_context.encode()).hexdigest()}"


# ------------------------------------------------
# ë²¡í„°DB ë¡œë“œ
# ------------------------------------------------
def load_vector_db(folder_path: str, base_name: str):
    t = time.time()
    index_path = os.path.join(folder_path, f"{base_name}.faiss")
    meta_path = os.path.join(folder_path, f"{base_name}_metadata.jsonl")

    if not os.path.exists(index_path) or not os.path.exists(meta_path):
        raise FileNotFoundError(f"[{base_name}] íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {folder_path}")

    index = faiss.read_index(index_path)
    with open(meta_path, "r", encoding="utf-8") as f:
        metadata = [json.loads(line.strip()) for line in f]

    print(f"â±ï¸ [load_vector_db] {base_name} ë¡œë“œ ì™„ë£Œ ({time.time() - t:.2f}s)")
    return index, metadata


# ------------------------------------------------
# ë²¡í„° ê²€ìƒ‰
# ------------------------------------------------
def retrieve_similar_docs(index, metadata, query_vector: np.ndarray, top_k: int = 5):
    t = time.time()
    D, I = index.search(query_vector, top_k)
    results = [metadata[idx] for idx in I[0] if 0 <= idx < len(metadata)]
    print(f"â±ï¸ [retrieve_similar_docs] ê²€ìƒ‰ ì™„ë£Œ ({time.time() - t:.2f}s)")
    return results


# ------------------------------------------------
# âœ… í…ìŠ¤íŠ¸ ì••ì¶•
# ------------------------------------------------
def compact_text(text: str) -> str:
    return " ".join(text.split())


# ------------------------------------------------
# âœ… í”„ë¡¬í”„íŠ¸ ìƒì„±
# ------------------------------------------------
def get_prompt_for_mode(mode: str, mct_id: str, combined_context: str) -> str:
    prompts = {
        "v1": f"""
        ë‹¤ìŒì€ '{mct_id}' ë§¤ì¥ê³¼ ìœ ì‚¬í•œ ì‚¬ë¡€ë“¤ì˜ **ê³ ê° ë¶„ì„ ë° ë§ˆì¼€íŒ… ë°ì´í„°**ì…ë‹ˆë‹¤.
        ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **AI ë§ˆì¼€íŒ… ë¦¬í¬íŠ¸**ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

        {combined_context}

        ì‘ì„± ì§€ì¹¨:
        1ï¸âƒ£ ë§¤ì¥ í•µì‹¬ ìš”ì•½ â€” ê³ ê° êµ¬ì„±, êµ¬ë§¤ íŒ¨í„´, ì£¼ìš” ìƒê¶Œ íŠ¹ì§•ì„ ìš”ì•½  
        2ï¸âƒ£ ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ â€” ìµœì†Œ 3ê°œ, ê° ì¸ì‚¬ì´íŠ¸ë§ˆë‹¤ **ë°ì´í„° ê·¼ê±°**ë¥¼ ê´„í˜¸ë¡œ ëª…ì‹œ  
        3ï¸âƒ£ íƒ€ê²Ÿì¸µë³„ë¡œ ì í•©í•œ ë§ˆì¼€íŒ… ì±„ë„ ì¶”ì²œ  
        4ï¸âƒ£ ì¶”ì²œ ì±„ë„ë³„ ë§ì¶¤ í™ë³´ ë¬¸êµ¬ ì œì•ˆ  
        5ï¸âƒ£ ê²°ë¡  â€” ì–´ë–¤ ì±„ë„ì´ ROI ëŒ€ë¹„ ê°€ì¥ íš¨ê³¼ì ì¸ì§€ ì œì‹œ
        """,

        "v2": f"""
        ë‹¤ìŒì€ '{mct_id}' ë§¤ì¥ê³¼ ìœ ì‚¬í•œ ì‚¬ë¡€ë“¤ì˜ **ì¬ë°©ë¬¸ìœ¨ ë¶„ì„ ë°ì´í„°**ì…ë‹ˆë‹¤.
        ì´ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **ì¬ë°©ë¬¸ìœ¨ í–¥ìƒ ì „ëµ ë³´ê³ ì„œ**ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

        {combined_context}
        """,

        "v3": f"""
        ë‹¤ìŒì€ '{mct_id}' ë§¤ì¥ê³¼ ìœ ì‚¬í•œ **ìš”ì‹ì—…ì¢… ê°€ë§¹ì  ë°ì´í„°**ì…ë‹ˆë‹¤.
        ì´ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **í˜„ì¬ ê°€ì¥ í° ë¬¸ì œì ê³¼ ì´ë¥¼ ë³´ì™„í•  ë§ˆì¼€íŒ… ì•„ì´ë””ì–´**ë¥¼ ì œì‹œí•˜ì„¸ìš”.

        {combined_context}
        """,

        "default": f"""
        ë‹¤ìŒì€ '{mct_id}' ë§¤ì¥ê³¼ ìœ ì‚¬í•œ ì‚¬ë¡€ë“¤ì˜ ë°ì´í„°ì…ë‹ˆë‹¤.
        ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ AI ë§ˆì¼€íŒ… ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

        {combined_context}
        """
    }
    return prompts.get(mode, prompts["default"])


# ------------------------------------------------
# âœ… Gemini í˜¸ì¶œ (ì¶œë ¥ 500í† í° ì œí•œ)
# ------------------------------------------------
model = genai.GenerativeModel("gemini-2.5-flash")

def generate_with_retry(prompt: str):
    """Gemini í˜¸ì¶œ â€” ì œí•œ ì—†ì´ ëê¹Œì§€ ê¸°ë‹¤ë¦¬ë˜, ì¶œë ¥ë§Œ 500í† í° ì œí•œ"""
    for attempt in range(2):
        try:
            return model.generate_content(
                prompt,
                generation_config={"max_output_tokens": 500}
            )
        except Exception as e:
            print(f"âš ï¸ Gemini í˜¸ì¶œ ì¬ì‹œë„ ì¤‘... ({attempt+1}/2) ì´ìœ : {e}")
            time.sleep(2)
    raise RuntimeError("Gemini í˜¸ì¶œ ì‹¤íŒ¨")


# ------------------------------------------------
# âœ… RAG ë¦¬í¬íŠ¸ ìƒì„±
# ------------------------------------------------
def generate_rag_summary(mct_id: str, mode: str = "v1", top_k: int = 5) -> Dict[str, Any]:
    print(f"\nğŸš€ [RAG Triggered] mct_id={mct_id}, mode={mode}")
    t_start = time.time()

    try:
        base_dir = os.path.dirname(os.path.abspath(__file__))  
        report_folder = os.path.join(base_dir, "vector_dbs", mode)
        shared_folder = os.path.join(base_dir, "vector_dbs", "shared")

        # 1ï¸âƒ£ ë²¡í„°DB ë¡œë“œ
        t1 = time.time()
        reports_index, reports_meta = load_vector_db(report_folder, "marketing_reports")
        segments_index, segments_meta = load_vector_db(shared_folder, "marketing_segments")
        print(f"â±ï¸ [1] ë²¡í„°DB ë¡œë“œ ì‹œê°„: {time.time() - t1:.2f}s")

        # 2ï¸âƒ£ ì„ë² ë”© ìƒì„±
        t2 = time.time()
        query_emb = embedder.encode([mct_id], normalize_embeddings=True, convert_to_numpy=True)
        query_vector = np.array(query_emb, dtype=np.float32)
        print(f"â±ï¸ [2] ì„ë² ë”© ìƒì„± ì‹œê°„: {time.time() - t2:.2f}s")

        # 3ï¸âƒ£ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
        t3 = time.time()
        report_results = retrieve_similar_docs(reports_index, reports_meta, query_vector, top_k)
        segment_results = retrieve_similar_docs(segments_index, segments_meta, query_vector, top_k)
        print(f"â±ï¸ [3] ê²€ìƒ‰ ì „ì²´ ì‹œê°„: {time.time() - t3:.2f}s")

        # 4ï¸âƒ£ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        t4 = time.time()
        report_context = compact_text("\n\n".join([r.get("text", "") for r in report_results]))
        segment_context = compact_text("\n\n".join([r.get("text", "") for r in segment_results]))
        combined_context = f"[ë§¤ì¥ ë¶„ì„ ë°ì´í„°]\n{report_context}\n\n[ë§ˆì¼€íŒ… ì „ëµ ë°ì´í„°]\n{segment_context}"
        print(f"â±ï¸ [4] ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì‹œê°„: {time.time() - t4:.2f}s")

        # 5ï¸âƒ£ í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = get_prompt_for_mode(mode, mct_id, combined_context)
        prompt_len = len(prompt)
        est_tokens = int(prompt_len / 4)
        print(f"ğŸ§¾ [Prompt Info] ê¸€ì ìˆ˜: {prompt_len:,} / ì˜ˆìƒ í† í° ìˆ˜: ì•½ {est_tokens:,}")

        # 6ï¸âƒ£ ìºì‹œ ì¡°íšŒ
        cache_key = get_cache_key(mct_id, mode, combined_context)
        if cache_key in _cache:
            print(f"âš¡ [CACHE HIT] '{mct_id}' ({mode}) ê²°ê³¼ ì¬ì‚¬ìš©")
            return _cache[cache_key]

        # 7ï¸âƒ£ Gemini í˜¸ì¶œ (ì¶œë ¥ ì œí•œ 500í† í°)
        t5 = time.time()
        response = generate_with_retry(prompt)
        print(f"â±ï¸ [5] Gemini í˜¸ì¶œ ì‹œê°„: {time.time() - t5:.2f}s")

        total_time = time.time() - t_start
        print(f"âœ… [ì´ ì†Œìš”ì‹œê°„] {total_time:.2f}s")

        result = {
            "store_code": mct_id,
            "rag_summary": response.text,
            "references": {"reports": report_results, "segments": segment_results},
            "timing": {
                "vector_load": round(time.time() - t1, 2),
                "embedding": round(time.time() - t2, 2),
                "search": round(time.time() - t3, 2),
                "context": round(time.time() - t4, 2),
                "gemini": round(time.time() - t5, 2),
                "total": round(total_time, 2)
            },
            "prompt_info": {
                "length": prompt_len,
                "estimated_tokens": est_tokens,
                "max_output_tokens": 500
            }
        }

        # 8ï¸âƒ£ ìºì‹œ ì €ì¥
        _cache[cache_key] = result
        print(f"ğŸ’¾ [CACHE STORE] '{mct_id}' ({mode}) ê²°ê³¼ ìºì‹± ì™„ë£Œ")

        return result

    except Exception as e:
        print("âŒ RAG ERROR:", e)
        return {"error": str(e), "traceback": traceback.format_exc(limit=2)}
