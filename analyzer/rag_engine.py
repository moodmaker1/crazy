"""
rag_engine.py
-------------
Gemini-2.5-Flash + FAISS ê¸°ë°˜ RAG ì—”ì§„
ëª¨ë¸ë³„ marketing_reports ë²¡í„°DB + ê³µí†µ marketing_segments ë²¡í„°DBë¥¼ ë³‘í•©í•˜ì—¬
AI ê¸°ë°˜ ë§ˆì¼€íŒ… ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

import os
import json
import traceback
import numpy as np
import faiss
from typing import Dict, Any
import google.generativeai as genai
from dotenv import load_dotenv

# ------------------------------------------------
# í™˜ê²½ ì„¤ì •
# ------------------------------------------------
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"

load_dotenv()
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
print("ğŸ” GEMINI_API_KEY =", "âœ… ë¡œë“œ ì™„ë£Œ" if os.getenv("GEMINI_API_KEY") else "âŒ ì—†ìŒ")

# ------------------------------------------------
# ë²¡í„°DB ë¡œë“œ ìœ í‹¸
# ------------------------------------------------
def load_vector_db(folder_path: str, base_name: str):
    """FAISS ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
    index_path = os.path.join(folder_path, f"{base_name}.faiss")
    meta_path = os.path.join(folder_path, f"{base_name}_metadata.jsonl")
    if not os.path.exists(index_path) or not os.path.exists(meta_path):
        raise FileNotFoundError(f"[{base_name}] íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {folder_path}")

    index = faiss.read_index(index_path)
    with open(meta_path, "r", encoding="utf-8") as f:
        metadata = [json.loads(line.strip()) for line in f]

    return index, metadata

# ------------------------------------------------
# ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
# ------------------------------------------------
def retrieve_similar_docs(index, metadata, query_vector: np.ndarray, top_k: int = 5):
    """ì¿¼ë¦¬ ë²¡í„°ë¡œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"""
    _, I = index.search(query_vector, top_k)
    return [metadata[i] for i in I[0] if 0 <= i < len(metadata)]

# ------------------------------------------------
# ëª¨ë“œë³„ í”„ë¡¬í”„íŠ¸
# ------------------------------------------------
def get_prompt_for_mode(mode: str, mct_id: str, combined_context: str) -> str:
    """ëª¨ë“œ(v1/v2/v3)ì— ë”°ë¼ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ ë°˜í™˜"""
    prompts = {
        "v1": f"""
        ë‹¤ìŒì€ '{mct_id}' ë§¤ì¥ê³¼ ìœ ì‚¬í•œ ì‚¬ë¡€ë“¤ì˜ **ê³ ê° ë¶„ì„ ë° ë§ˆì¼€íŒ… ë°ì´í„°**ì…ë‹ˆë‹¤.
        ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **AI ë§ˆì¼€íŒ… ë¦¬í¬íŠ¸**ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

        {combined_context}

        ì‘ì„± ì§€ì¹¨:
        1ï¸âƒ£ ë§¤ì¥ í•µì‹¬ ìš”ì•½ â€” ê³ ê° êµ¬ì„±, êµ¬ë§¤ íŒ¨í„´, ì£¼ìš” ìƒê¶Œ íŠ¹ì§•
        2ï¸âƒ£ íƒ€ê²Ÿì¸µë³„ ì í•© ì±„ë„ ì¶”ì²œ ë° í™ë³´ ë¬¸êµ¬ ì œì•ˆ
        """,

        "v2": f"""
        ë‹¤ìŒì€ '{mct_id}' ë§¤ì¥ê³¼ ìœ ì‚¬í•œ ì‚¬ë¡€ë“¤ì˜ **ì¬ë°©ë¬¸ìœ¨ ë¶„ì„ ë°ì´í„°**ì…ë‹ˆë‹¤.
        ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **ì¬ë°©ë¬¸ìœ¨ í–¥ìƒ ì „ëµ ë¦¬í¬íŠ¸**ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

        {combined_context}

        ì‘ì„± ì§€ì¹¨:
        1ï¸âƒ£ ë‹¨ê¸°/ì¤‘ê¸°/ì¥ê¸° ë¦¬í…ì…˜ ì „ëµ êµ¬ì²´í™”
        2ï¸âƒ£ ë§¤ì¥ì—ì„œ í˜„ì¬ ì¬ë°©ë¬¸ë¥ ì„ ë†’ì¼ ìˆ˜ ìˆëŠ” ë§ˆì¼€íŒ… ì•„ì´ë””ì–´ì™€ ê·¼ê±°ë¥¼ ì œì‹œ
        """,

        "v3": f"""
        ë‹¤ìŒì€ '{mct_id}' ë§¤ì¥ê³¼ ìœ ì‚¬í•œ **ìš”ì‹ì—…ì¢… ê°€ë§¹ì  ë°ì´í„°**ì…ë‹ˆë‹¤.
        ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **ë¬¸ì œ ì§„ë‹¨ ë° ê°œì„  ì•„ì´ë””ì–´**ë¥¼ ì œì‹œí•˜ì„¸ìš”.

        {combined_context}

        ì‘ì„± ì§€ì¹¨:
        1ï¸âƒ£ ë¬¸ì œë³„ ì›ì¸ ë¶„ì„ ë° íŠ¸ë Œë“œ ì—°ê´€
        2ï¸âƒ£ ê°œì„  ì•„ì´ë””ì–´ 3ê°œ ì œì‹œ (ì˜¨/ì˜¤í”„ë¼ì¸)
        """,
    }
    return prompts.get(mode, prompts["v1"])

# ------------------------------------------------
# RAG ë¦¬í¬íŠ¸ ìƒì„±
# ------------------------------------------------
def generate_rag_summary(mct_id: str, mode: str = "v1", top_k: int = 5) -> Dict[str, Any]:
    """Gemini ê¸°ë°˜ RAG ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
    print(f"ğŸš€ [RAG Triggered] mct_id={mct_id}, mode={mode}")

    try:
        base_dir = os.path.dirname(os.path.abspath(__file__))
        report_folder = os.path.join(base_dir, "vector_dbs", mode)
        shared_folder = os.path.join(base_dir, "vector_dbs", "shared")

        # âœ… ë²¡í„°DB ë¡œë“œ
        reports_index, reports_meta = load_vector_db(report_folder, "marketing_reports")
        segments_index, segments_meta = load_vector_db(shared_folder, "marketing_segments")

        # âœ… ì„ë² ë”© ìƒì„±
        from sentence_transformers import SentenceTransformer
        embedder = SentenceTransformer("BAAI/bge-m3")
        query_emb = embedder.encode([mct_id], normalize_embeddings=True)
        query_vector = np.array(query_emb, dtype=np.float32)

        # âœ… ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
        report_results = retrieve_similar_docs(reports_index, reports_meta, query_vector, top_k)
        segment_results = retrieve_similar_docs(segments_index, segments_meta, query_vector, top_k)

        if not report_results and not segment_results:
            return {"error": f"'{mct_id}' ê´€ë ¨ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

        # âœ… ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        report_context = "\n\n".join([r.get("text", "") for r in report_results])
        segment_context = "\n\n".join([r.get("text", "") for r in segment_results])
        combined_context = f"[ë§¤ì¥ ë¶„ì„ ë°ì´í„°]\n{report_context}\n\n[ë§ˆì¼€íŒ… ì „ëµ ë°ì´í„°]\n{segment_context}"

        # âœ… í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = get_prompt_for_mode(mode, mct_id, combined_context)

        # âœ… Gemini í˜¸ì¶œ
        model = genai.GenerativeModel("gemini-2.5-flash")
        response = model.generate_content(prompt)

        return {
            "store_code": mct_id,
            "rag_summary": response.text,
            "references": {
                "reports": report_results,
                "segments": segment_results,
            },
        }

    except Exception as e:
        print("âŒ RAG ERROR:", e)
        return {"error": str(e), "traceback": traceback.format_exc(limit=2)}