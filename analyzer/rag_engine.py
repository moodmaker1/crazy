"""
rag_engine.py
-------------
Gemini-2.5-Flash + FAISS ê¸°ë°˜ RAG ì—”ì§„
- ì£¼ ê³ ê°ì¸µ ê°•í™” ì „ëµ + ìœ ì‚¬ë§¤ì¥ íƒ€ê²Ÿ í™•ì¥ ì „ëµ ë³‘í•©í˜• ë¶„ì„
- í˜„ì¬ ë§¤ì¥ í˜ë¥´ì†Œë‚˜(summary/persona ë“±)ë¥¼ í”„ë¡¬í”„íŠ¸ ì»¨í…ìŠ¤íŠ¸ ìµœìƒë‹¨ì— ì•µì»¤ë¡œ ì‚½ì…
"""
import warnings
warnings.filterwarnings("ignore", message="resource_tracker")
import os
import json
import traceback
import threading
import time
import numpy as np
import faiss
from typing import Dict, Any, List, Tuple
import google.generativeai as genai
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
import multiprocessing

# ------------------------------------------------
# âœ… ë³‘ë ¬/ì„±ëŠ¥ ì„¤ì •
# ------------------------------------------------
num_cores = min(4, multiprocessing.cpu_count())
os.environ["OMP_NUM_THREADS"] = str(num_cores)
os.environ["MKL_NUM_THREADS"] = str(num_cores)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
print(f"âš™ï¸ ë³‘ë ¬ ì„¤ì •: OMP={num_cores}, MKL={num_cores}")

# ------------------------------------------------
# âœ… í™˜ê²½ ë³€ìˆ˜ ë° ëª¨ë¸ ì´ˆê¸°í™”
# ------------------------------------------------
load_dotenv()
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
print("ğŸ” GEMINI_API_KEY =", "âœ… ë¡œë“œ ì™„ë£Œ" if os.getenv("GEMINI_API_KEY") else "âŒ ì—†ìŒ")

embedder = None
_embedder_lock = threading.Lock()

def _load_embedder_background():
    global embedder
    try:
        with _embedder_lock:
            if embedder is None:
                print("ğŸš€ [Init] ì„ë² ë”© ëª¨ë¸ ë°±ê·¸ë¼ìš´ë“œ ë¡œë“œ ì‹œì‘...")
                t0 = time.time()
                embedder = SentenceTransformer("BAAI/bge-m3")
                print(f"âœ… [Init] ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ì „ì—­ 1íšŒ, {time.time() - t0:.2f}s)")
    except Exception as e:
        print("âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨:", e)

threading.Thread(target=_load_embedder_background, daemon=True).start()


# ------------------------------------------------
# ë²¡í„°DB ë¡œë“œ ìœ í‹¸
# ------------------------------------------------
def load_vector_db(folder_path: str, base_name: str):
    t0 = time.time()
    index_path = os.path.join(folder_path, f"{base_name}.faiss")
    meta_path = os.path.join(folder_path, f"{base_name}_metadata.jsonl")
    if not os.path.exists(index_path) or not os.path.exists(meta_path):
        raise FileNotFoundError(f"[{base_name}] íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {folder_path}")
    index = faiss.read_index(index_path)
    with open(meta_path, "r", encoding="utf-8") as f:
        metadata = [json.loads(line.strip()) for line in f]
    print(f"â±ï¸ [load_vector_db] {base_name} ë¡œë“œ ì™„ë£Œ ({time.time() - t0:.2f}s)")
    return index, metadata


# ------------------------------------------------
# ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
# ------------------------------------------------
def retrieve_similar_docs(index, metadata, query_vector: np.ndarray, top_k: int = 5):
    t0 = time.time()
    D, I = index.search(query_vector, top_k)
    results = [metadata[idx] for idx in I[0] if 0 <= idx < len(metadata)]
    print(f"â±ï¸ [retrieve_similar_docs] ê²€ìƒ‰ ì™„ë£Œ ({time.time() - t0:.2f}s)")
    return results


# ------------------------------------------------
# (ê°œì„ A) ë“€ì–¼ ì¿¼ë¦¬ â€” ìš°ë¦¬ ë§¤ì¥ ê°•í™” + ìœ ì‚¬ë§¤ì¥ í™•ì¥
# ------------------------------------------------
def build_dual_queries(mct_id: str, mode: str) -> List[str]:
    """ë§¤ì¥ ì¤‘ì‹¬ ì¿¼ë¦¬ + ìœ ì‚¬ë§¤ì¥ íƒ€ê²Ÿ ì „ëµ ì¿¼ë¦¬ ë™ì‹œ ìˆ˜í–‰"""
    base_intent = {
        "v1": "ê³ ê° ë¶„ì„, ì£¼ìš” ê³ ê°ì¸µ, ìƒê¶Œ íŠ¹ì§•, ì±„ë„ ì„±ê³¼",
        "v2": "ì¬ë°©ë¬¸ìœ¨, ë¦¬í…ì…˜, ë©¤ë²„ì‹­, í‘¸ì‹œ ì „ëµ",
        "v3": "ë¬¸ì œ ì§„ë‹¨, ì›ì¸ ë¶„ì„, ê°œì„  ì•„ì´ë””ì–´",
    }.get(mode, "ë§¤ì¥ ë¶„ì„, ë§ˆì¼€íŒ… ì „ëµ, ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸")

    query_1 = f"{mct_id} ë§¤ì¥ì˜ {base_intent} ë° ì£¼ ê³ ê°ì¸µ ê°•í™” ì „ëµ"  # ìš°ë¦¬ ë§¤ì¥ ì¤‘ì‹¬
    query_2 = (
        "ìœ ì‚¬ ë§¤ì¥ì—ì„œ ì„±ê³µí•œ íƒ€ê²Ÿ í™•ì¥ ì „ëµ, ì—°ë ¹/ì„±ë³„ë³„ íƒ€ê²ŸíŒ…, "
        "ì±„ë„ë³„ ì„±ê³¼, íŠ¸ë Œë“œ ê¸°ë°˜ ë§ˆì¼€íŒ… ì‚¬ë¡€"  # í™•ì¥ íƒ€ê²Ÿ ì°¸ê³ 
    )
    return [query_1, query_2]


# ------------------------------------------------
# (ê°œì„ B) í˜„ì¬ ë§¤ì¥ í˜ë¥´ì†Œë‚˜/ìš”ì•½ ì•µì»¤ ìƒì„±
# ------------------------------------------------
def build_store_profile_anchor(report_results: List[dict]) -> str:
    """
    report_results ìƒë‹¨ì—ì„œ summary/persona/visit_mix/loyalty ë“±ì„ ì¶”ì¶œí•´
    í”„ë¡¬í”„íŠ¸ ì»¨í…ìŠ¤íŠ¸ ìµœìƒë‹¨ì— ê³ ì •(ì•µì»¤) ì‚½ì….
    """
    if not report_results:
        return ""
    cand = report_results[0]  # ê´€ë¡€ìƒ 0ë²ˆì§¸ê°€ í•´ë‹¹ ë§¤ì¥/í•µì‹¬ ë¬¸ë§¥ì¼ í™•ë¥ ì´ ê°€ì¥ ë†’ìŒ
    fields = []
    if cand.get("summary"):
        fields.append(f"summary: {cand['summary']}")
    if cand.get("persona"):
        fields.append(f"persona: {cand['persona']}")
    if cand.get("visit_mix"):
        fields.append(f"visit_mix: {cand['visit_mix']}")
    if cand.get("loyalty"):
        fields.append(f"loyalty: {cand['loyalty']}")
    if not fields:
        return ""
    return "ğŸ“Š [í˜„ì¬ ë§¤ì¥ ë°ì´í„° ë¶„ì„]\n" + "\n".join(fields) + "\n"


# ------------------------------------------------
# (ë³´ì¡°) ë¼ì¸ ì¤‘ë³µ ì œê±°
# ------------------------------------------------
def dedupe_lines(text: str) -> str:
    lines, seen, out = text.splitlines(), set(), []
    for ln in lines:
        if ln not in seen:
            seen.add(ln)
            out.append(ln)
    return "\n".join(out)


# ------------------------------------------------
# í”„ë¡¬í”„íŠ¸ (v1/v2/v3 ì‹¤í–‰í˜•ìœ¼ë¡œ í†µì¼)
# ------------------------------------------------
def get_prompt_for_mode(mode: str, mct_id: str, combined_context: str) -> str:
    """
    v1: ìš°ë¦¬ ë§¤ì¥ ê³ ê°ì¸µ ê°•í™” + ìœ ì‚¬ë§¤ì¥ ê¸°ë°˜ í™•ì¥ íƒ€ê²Ÿì„ í•¨ê»˜ ì œì‹œ (ì±„ë„/ë¬¸êµ¬ í¬í•¨)
    v2: ì¬ë°©ë¬¸ 30% ì´í•˜ ì ì£¼ ì¦‰ì‹œ ì‹¤í–‰ ì•„ì´ë””ì–´
    v3: ìš”ì‹ì—… ë¬¸ì œ ì§„ë‹¨ + ê°œì„  ì•„ì´ë””ì–´ (ë¬¸ì œ-í•´ê²°-ë¬¸êµ¬-ê·¼ê±°)
    """
    prompts = {
        # âœ… v1 â€” í˜ë¥´ì†Œë‚˜ ì•µì»¤ ìš°ì„ , ë‘ ì¶• ë³‘í–‰ (ê°•í™” + í™•ì¥)
        "v1": f"""
# {mct_id} ê³ ê° íŠ¹ì„± ê¸°ë°˜ ì±„ë„ ì¶”ì²œ & í™ë³´ ì‹¤í–‰ ê°€ì´ë“œ (ê°•í™” + í™•ì¥)

ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ë˜, **ë°˜ë“œì‹œ í˜„ì¬ ë§¤ì¥ ë°ì´í„°(ğŸ“Š)ë¥¼ ìµœìš°ì„ **ìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”.
ìœ ì‚¬ ë§¤ì¥ ë°ì´í„°ëŠ” ì°¸ê³ ìš©ì´ë©°, ê²°ê³¼ì—ëŠ” **[A] í˜„ì¬ ê³ ê°ì¸µ ê°•í™” ì „ëµ**ê³¼ **[B] ìœ ì‚¬ë§¤ì¥ ê¸°ë°˜ í™•ì¥ íƒ€ê²Ÿ ì „ëµ**ì„ í•¨ê»˜ ì œì‹œí•©ë‹ˆë‹¤.

{combined_context}

âš ï¸ **í•„ìˆ˜ ì¤€ìˆ˜ ì‚¬í•­ - ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì„ ì •í™•íˆ ë”°ë¥´ì„¸ìš”:**

[A] í˜„ì¬ ê³ ê°ì¸µ ê°•í™” ì „ëµ

1. [ì „ëµ ì œëª©]
ì¶”ì²œ ì±„ë„: [ì±„ë„ëª…, 1ì¤„]
í™ë³´ ë¬¸êµ¬ ì˜ˆì‹œ: [íƒ€ê²Ÿ ê³µê° ë¬¸ì¥, 1ì¤„]
ì‹¤í–‰ ë°©ë²•: [êµ¬ì²´ì  í–‰ë™, 1-2ì¤„]
ê·¼ê±°: [ìœ ì‚¬ ì‚¬ë¡€/ë°ì´í„°, 1ì¤„]

2. [ì „ëµ ì œëª©]
ì¶”ì²œ ì±„ë„: [ì±„ë„ëª…, 1ì¤„]
í™ë³´ ë¬¸êµ¬ ì˜ˆì‹œ: [íƒ€ê²Ÿ ê³µê° ë¬¸ì¥, 1ì¤„]
ì‹¤í–‰ ë°©ë²•: [êµ¬ì²´ì  í–‰ë™, 1-2ì¤„]
ê·¼ê±°: [ìœ ì‚¬ ì‚¬ë¡€/ë°ì´í„°, 1ì¤„]

[B] ìœ ì‚¬ë§¤ì¥ ê¸°ë°˜ í™•ì¥ íƒ€ê²Ÿ ì „ëµ

1. [ì „ëµ ì œëª©]
ì¶”ì²œ ì±„ë„: [ì±„ë„ëª…, 1ì¤„]
í™ë³´ ë¬¸êµ¬ ì˜ˆì‹œ: [íƒ€ê²Ÿ ê³µê° ë¬¸ì¥, 1ì¤„]
ì‹¤í–‰ ë°©ë²•: [êµ¬ì²´ì  í–‰ë™, 1-2ì¤„]
ê·¼ê±°: [ìœ ì‚¬ ì‚¬ë¡€/ë°ì´í„°, 1ì¤„]

2. [ì „ëµ ì œëª©]
ì¶”ì²œ ì±„ë„: [ì±„ë„ëª…, 1ì¤„]
í™ë³´ ë¬¸êµ¬ ì˜ˆì‹œ: [íƒ€ê²Ÿ ê³µê° ë¬¸ì¥, 1ì¤„]
ì‹¤í–‰ ë°©ë²•: [êµ¬ì²´ì  í–‰ë™, 1-2ì¤„]
ê·¼ê±°: [ìœ ì‚¬ ì‚¬ë¡€/ë°ì´í„°, 1ì¤„]

ì¤‘ìš”:
- "[A]"ì™€ "[B]" í—¤ë”ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”
- "ì¶”ì²œ ì±„ë„:", "í™ë³´ ë¬¸êµ¬ ì˜ˆì‹œ:", "ì‹¤í–‰ ë°©ë²•:", "ê·¼ê±°:" ë ˆì´ë¸”ì„ ì •í™•íˆ ì‚¬ìš©í•˜ì„¸ìš”
- ê° í•­ëª©ì€ ë°˜ë“œì‹œ "1.", "2."ë¡œ ì‹œì‘í•˜ì„¸ìš”
""",

        # âœ… v2 â€” ì ì£¼ ì¦‰ì‹œ ì‹¤í–‰ (ì¬ë°©ë¬¸ 30% ì´í•˜) - ë‹¨ê¸°/ì¤‘ê¸°/ì¥ê¸° ì „ëµ
        "v2": f"""
# ğŸ” {mct_id} ì¬ë°©ë¬¸ìœ¨ í–¥ìƒ ì „ëµ ìš”ì•½ & ì‹¤ì²œ ê°€ì´ë“œ

ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬, **ì¬ë°©ë¬¸ìœ¨ì´ 30% ì´í•˜ì¸ ë§¤ì¥**ì˜ ì ì£¼ê°€ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ì „ëµì„ ë‹¨ê¸°/ì¤‘ê¸°/ì¥ê¸°ë¡œ ë‚˜ëˆ„ì–´ ì œì‹œí•˜ì„¸ìš”.

{combined_context}

âš ï¸ **í•„ìˆ˜ ì¤€ìˆ˜ ì‚¬í•­ - ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì„ ì •í™•íˆ ë”°ë¥´ì„¸ìš”:**

[ë‹¨ê¸° ì „ëµ] (1-2ì£¼ ë‚´ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥)

1. [ì „ëµ ì œëª©]
ì‹¤í–‰ ë°©ë²•: [êµ¬ì²´ì  í–‰ë™ê³¼ ì˜ˆìƒ íš¨ê³¼, 2-3ì¤„]
ê·¼ê±°: [ìœ ì‚¬ ì‚¬ë¡€/ë°ì´í„° 1ì¤„]

2. [ì „ëµ ì œëª©]
ì‹¤í–‰ ë°©ë²•: [êµ¬ì²´ì  í–‰ë™ê³¼ ì˜ˆìƒ íš¨ê³¼, 2-3ì¤„]
ê·¼ê±°: [ìœ ì‚¬ ì‚¬ë¡€/ë°ì´í„° 1ì¤„]

[ì¤‘ê¸° ì „ëµ] (1-2ê°œì›” ë‚´ ì‹¤í–‰)

1. [ì „ëµ ì œëª©]
ì‹¤í–‰ ë°©ë²•: [êµ¬ì²´ì  í–‰ë™ê³¼ ì˜ˆìƒ íš¨ê³¼, 2-3ì¤„]
ê·¼ê±°: [ìœ ì‚¬ ì‚¬ë¡€/ë°ì´í„° 1ì¤„]

2. [ì „ëµ ì œëª©]
ì‹¤í–‰ ë°©ë²•: [êµ¬ì²´ì  í–‰ë™ê³¼ ì˜ˆìƒ íš¨ê³¼, 2-3ì¤„]
ê·¼ê±°: [ìœ ì‚¬ ì‚¬ë¡€/ë°ì´í„° 1ì¤„]

[ì¥ê¸° ì „ëµ] (3ê°œì›” ì´ìƒ ì§€ì† ì‹¤í–‰)

1. [ì „ëµ ì œëª©]
ì‹¤í–‰ ë°©ë²•: [êµ¬ì²´ì  í–‰ë™ê³¼ ì˜ˆìƒ íš¨ê³¼, 2-3ì¤„]
ê·¼ê±°: [ìœ ì‚¬ ì‚¬ë¡€/ë°ì´í„° 1ì¤„]

2. [ì „ëµ ì œëª©]
ì‹¤í–‰ ë°©ë²•: [êµ¬ì²´ì  í–‰ë™ê³¼ ì˜ˆìƒ íš¨ê³¼, 2-3ì¤„]
ê·¼ê±°: [ìœ ì‚¬ ì‚¬ë¡€/ë°ì´í„° 1ì¤„]

ì¤‘ìš”:
- "[ë‹¨ê¸° ì „ëµ]", "[ì¤‘ê¸° ì „ëµ]", "[ì¥ê¸° ì „ëµ]" í—¤ë”ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”
- "ì‹¤í–‰ ë°©ë²•:" ê³¼ "ê·¼ê±°:" ë ˆì´ë¸”ì„ ì •í™•íˆ ì‚¬ìš©í•˜ì„¸ìš”
- ê° ì„¹ì…˜ ë‚´ í•­ëª©ì€ ë°˜ë“œì‹œ "1.", "2."ë¡œ ì‹œì‘í•˜ì„¸ìš”
- ë¶„ì„ì´ë‚˜ ì„œë¡  ì—†ì´ ë°”ë¡œ [ë‹¨ê¸° ì „ëµ]ë¶€í„° ì‹œì‘í•˜ì„¸ìš”
- ë‹¨ê¸°ëŠ” ì ì€ ë¹„ìš©ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì‹¤í–‰ ê°€ëŠ¥í•œ ê²ƒ, ì¤‘ê¸°ëŠ” ì¼ì • íˆ¬ì í•„ìš”, ì¥ê¸°ëŠ” ì§€ì†ì  ë…¸ë ¥ í•„ìš”í•œ ì „ëµìœ¼ë¡œ êµ¬ë¶„í•˜ì„¸ìš”
""",

        # âœ… v3 â€” ë¬¸ì œ ì§„ë‹¨ + ê°œì„  (ë¬¸êµ¬ í¬í•¨)
        "v3": f"""
# ğŸ½ï¸ {mct_id} ìš”ì‹ì—… ë§¤ì¥ ë¬¸ì œ ì§„ë‹¨ ë° ê°œì„  ì•„ì´ë””ì–´ ê°€ì´ë“œ

ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ë˜, **í˜„ì¬ ë§¤ì¥ ìƒí™©/ê³ ê° íŠ¹ì„±/í˜„ì¬ ë§¤ì¥ ì—…ì¢…**ì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì œë¥¼ ì§„ë‹¨í•˜ê³ , **ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ê°œì„  ì•„ì´ë””ì–´**ë¥¼ ì œì‹œí•˜ì„¸ìš”.

{combined_context}

âš ï¸ **í•„ìˆ˜ ì¤€ìˆ˜ ì‚¬í•­ - ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì„ ì •í™•íˆ ë”°ë¥´ì„¸ìš”:**

1. [ê°œì„  ì „ëµ ì œëª©]
ì‹¤í–‰ ë°©ë²•: [ë¬¸ì œ ì§„ë‹¨,ê°œì„  í–‰ë™,ì˜ˆìƒ íš¨ê³¼ì„ í¬í•¨í•˜ì—¬ 2~3ì¤„ë¡œ ì‘ì„±í•˜ì„¸ìš”.]
ê·¼ê±°: [ìœ ì‚¬ ì‚¬ë¡€/ë°ì´í„° 1ì¤„]

2. [ê°œì„  ì „ëµ ì œëª©]
ì‹¤í–‰ ë°©ë²•: [ë¬¸ì œ ì§„ë‹¨,ê°œì„  í–‰ë™,ì˜ˆìƒ íš¨ê³¼ì„ í¬í•¨í•˜ì—¬ 2~3ì¤„ë¡œ ì‘ì„±í•˜ì„¸ìš”.]
ê·¼ê±°: [ìœ ì‚¬ ì‚¬ë¡€/ë°ì´í„° 1ì¤„]

3. [ê°œì„  ì „ëµ ì œëª©]
ì‹¤í–‰ ë°©ë²•: [ë¬¸ì œ ì§„ë‹¨,ê°œì„  í–‰ë™,ì˜ˆìƒ íš¨ê³¼ì„ í¬í•¨í•˜ì—¬ 2~3ì¤„ë¡œ ì‘ì„±í•˜ì„¸ìš”.]
ê·¼ê±°: [ìœ ì‚¬ ì‚¬ë¡€/ë°ì´í„° 1ì¤„]



ì¤‘ìš”:
- "ì‹¤í–‰ ë°©ë²•:" ê³¼ "ê·¼ê±°:" ë ˆì´ë¸”ì„ ì •í™•íˆ ì‚¬ìš©í•˜ì„¸ìš”
- ê° í•­ëª©ì€ ë°˜ë“œì‹œ "1.", "2.", "3."ë¡œ ì‹œì‘í•˜ì„¸ìš”
- ë¶„ì„ì´ë‚˜ ì„œë¡  ì—†ì´ ë°”ë¡œ 1ë²ˆë¶€í„° ì‹œì‘í•˜ì„¸ìš”
- ê° ë¬¸ì¥ì€ ë°˜ë“œì‹œ ì¤„ë°”ê¿ˆ(Enter)ìœ¼ë¡œ êµ¬ë¶„í•˜ì„¸ìš”.
""",
    }
    return prompts.get(mode, prompts["v1"])


# ------------------------------------------------
# RAG ë¦¬í¬íŠ¸ ìƒì„±
# ------------------------------------------------
def generate_rag_summary(mct_id: str, mode: str = "v1", top_k: int = 5) -> Dict[str, Any]:
    t_start = time.time()
    print(f"ğŸš€ [RAG Triggered] mct_id={mct_id}, mode={mode}")

    try:
        # 1) ë²¡í„°DB ë¡œë“œ
        base_dir = os.path.dirname(os.path.abspath(__file__))
        report_folder = os.path.join(base_dir, "vector_dbs", mode)
        shared_folder = os.path.join(base_dir, "vector_dbs", "shared")
        reports_index, reports_meta = load_vector_db(report_folder, "marketing_reports")
        segments_index, segments_meta = load_vector_db(shared_folder, "marketing_segments")

        # 2) ì„ë² ë”© ì¤€ë¹„
        global embedder
        if embedder is None:
            _load_embedder_background()
            while embedder is None:
                time.sleep(0.5)

        # 3) ë“€ì–¼ ì¿¼ë¦¬ ê²€ìƒ‰ (ìš°ë¦¬ ë§¤ì¥ ê°•í™” + ìœ ì‚¬ë§¤ì¥ í™•ì¥)
        queries = build_dual_queries(mct_id, mode)
        all_reports, all_segments = [], []
        for q in queries:
            q_emb = embedder.encode([q], normalize_embeddings=True)
            q_vec = np.array(q_emb, dtype=np.float32)
            all_reports.extend(retrieve_similar_docs(reports_index, reports_meta, q_vec, top_k))
            all_segments.extend(retrieve_similar_docs(segments_index, segments_meta, q_vec, top_k))

        # 4) (ê°„ë‹¨) ì¤‘ë³µ ì œê±°
        def _uniq(items: List[dict], key_priority: List[str]) -> List[dict]:
            seen, out = set(), []
            for i, it in enumerate(items):
                key = None
                for k in key_priority:
                    if it.get(k) is not None:
                        key = f"{k}:{it.get(k)}"
                        break
                if key is None:
                    key = f"idx:{i}"
                if key not in seen:
                    seen.add(key)
                    out.append(it)
            return out

        report_results = _uniq(all_reports, ["id", "chunk_id", "store_code"])
        segment_results = _uniq(all_segments, ["id", "chunk_id", "store_code"])

        if not report_results and not segment_results:
            return {"error": f"'{mct_id}' ê´€ë ¨ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

        # 5) í˜ë¥´ì†Œë‚˜ ì•µì»¤ êµ¬ì„±
        persona_anchor = build_store_profile_anchor(report_results)

        # 6) ì»¨í…ìŠ¤íŠ¸ ë³‘í•© (ì•µì»¤ â†’ ìš°ë¦¬ ë§¤ì¥ ë°ì´í„° â†’ ìœ ì‚¬ ë§¤ì¥ ì‚¬ë¡€)
        report_context = "\n\n".join([r.get("text", "") for r in report_results])
        segment_context = "\n\n".join([s.get("text", "") for s in segment_results])

        combined_context = ""
        if persona_anchor:
            combined_context += persona_anchor + "\n"
        combined_context += (
            "[ë§¤ì¥ ì£¼ìš” ë¶„ì„ ë° ê³ ê°ì¸µ ê°•í™” ë°ì´í„°]\n"
            + (report_context or "(ë°ì´í„° ì—†ìŒ)") + "\n\n"
            + "[ìœ ì‚¬ ë§¤ì¥ íƒ€ê²Ÿ í™•ì¥ ì „ëµ ì‚¬ë¡€]\n"
            + (segment_context or "(ë°ì´í„° ì—†ìŒ)")
        )
        combined_context = dedupe_lines(combined_context)

        # 7) í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = get_prompt_for_mode(mode, mct_id, combined_context)
        print(f"ğŸ§¾ [Prompt Info] ê¸€ì ìˆ˜: {len(prompt):,} / ì˜ˆìƒ í† í° ìˆ˜: ~{len(prompt)//4}")

        # 8) Gemini í˜¸ì¶œ
        t4 = time.time()
        model = genai.GenerativeModel("gemini-2.5-flash")
        response = model.generate_content(prompt)
        print(f"â±ï¸ [Gemini í˜¸ì¶œ ì‹œê°„] {time.time() - t4:.2f}s")
        print(f"âœ… [ì´ ì†Œìš”ì‹œê°„] {time.time() - t_start:.2f}s")

        return {
            "store_code": mct_id,
            "rag_summary": response.text,
            "references": {"reports": report_results, "segments": segment_results},
        }

    except Exception as e:
        print(f"âŒ RAG ERROR: {e}")
        return {"error": str(e), "traceback": traceback.format_exc(limit=2)}
