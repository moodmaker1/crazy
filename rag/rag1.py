"""
RAG ê¸°ë°˜ ë§¤ì¥ë³„ ë§ˆì¼€íŒ… ì±„ë„ ì¶”ì²œ ì‹œìŠ¤í…œ (Google Gemini 2.5 Flash ë²„ì „)
- PyTrendsë¡œ ì—…ì¢… íŠ¸ë Œë“œ ë¶„ì„
- ì—‘ì…€ íŒŒì¼ì—ì„œ ë§ˆì¼€íŒ… ì „ëµ ë¬¸ì„œ ë¡œë“œ
- ë§¤ì¥ íŠ¹ì„±ê³¼ ë¬¸ì„œ ê°„ ìœ ì‚¬ë„ ê³„ì‚°
- Gemini 2.5 Flashë¡œ ë§ì¶¤í˜• ë§ˆì¼€íŒ… ì „ëµ ìƒì„± (4ê°œ ì„¹ì…˜)
"""

import os
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import json
from pathlib import Path
import warnings
import google.generativeai as genai
from pytrends.request import TrendReq

warnings.filterwarnings('ignore')

class RAGMarketingSystem:
    def __init__(self, data_folder_path, gemini_api_key):
        """
        Args:
            data_folder_path: ragë°ì´í„°_ë³‘ì§„ë‹˜ í´ë” ê²½ë¡œ
            gemini_api_key: Google Gemini API í‚¤
        """
        self.data_folder = Path(data_folder_path)
        
        # Sentence Transformer ëª¨ë¸ (ê²€ìƒ‰ìš©)
        print("ğŸ”„ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        
        # Gemini ì„¤ì •
        genai.configure(api_key=gemini_api_key)
        self.gemini_model = genai.GenerativeModel('gemini-2.0-flash-exp')
        
        # PyTrends ì„¤ì •
        self.pytrends = TrendReq(hl='ko', tz=540)
        
        self.documents = []
        self.doc_embeddings = None
        
    def get_industry_trends(self, industry_keyword, timeframe='today 3-m'):
        """PyTrendsë¡œ ì—…ì¢… ê²€ìƒ‰ íŠ¸ë Œë“œ ê°€ì ¸ì˜¤ê¸°"""
        try:
            print(f"ğŸ“ˆ '{industry_keyword}' ì—…ì¢… íŠ¸ë Œë“œ ë¶„ì„ ì¤‘...")
            
            # í‚¤ì›Œë“œ ì„¤ì •
            self.pytrends.build_payload([industry_keyword], timeframe=timeframe, geo='KR')
            
            # ì‹œê°„ë³„ ê´€ì‹¬ë„
            interest_over_time = self.pytrends.interest_over_time()
            
            # ì§€ì—­ë³„ ê´€ì‹¬ë„
            interest_by_region = self.pytrends.interest_by_region(resolution='REGION', inc_low_vol=True)
            
            # ê´€ë ¨ ê²€ìƒ‰ì–´
            related_queries = self.pytrends.related_queries()
            
            # ë°ì´í„° ìš”ì•½
            trend_summary = {
                'keyword': industry_keyword,
                'current_trend': 'N/A',
                'avg_interest': 0,
                'top_regions': [],
                'rising_queries': []
            }
            
            if not interest_over_time.empty:
                trend_summary['avg_interest'] = int(interest_over_time[industry_keyword].mean())
                latest_value = interest_over_time[industry_keyword].iloc[-1]
                prev_value = interest_over_time[industry_keyword].iloc[-5] if len(interest_over_time) > 5 else latest_value
                
                if latest_value > prev_value * 1.1:
                    trend_summary['current_trend'] = 'ìƒìŠ¹ì„¸ ğŸ“ˆ'
                elif latest_value < prev_value * 0.9:
                    trend_summary['current_trend'] = 'í•˜ë½ì„¸ ğŸ“‰'
                else:
                    trend_summary['current_trend'] = 'ì•ˆì •ì„¸ â¡ï¸'
            
            if not interest_by_region.empty:
                top_regions = interest_by_region.nlargest(3, industry_keyword)
                trend_summary['top_regions'] = [
                    f"{region} ({int(value)})" 
                    for region, value in zip(top_regions.index, top_regions[industry_keyword])
                ]
            
            if related_queries[industry_keyword]['rising'] is not None:
                rising = related_queries[industry_keyword]['rising'].head(5)
                trend_summary['rising_queries'] = rising['query'].tolist() if not rising.empty else []
            
            print(f"  âœ“ íŠ¸ë Œë“œ: {trend_summary['current_trend']}")
            print(f"  âœ“ í‰ê·  ê´€ì‹¬ë„: {trend_summary['avg_interest']}/100")
            print()
            
            return trend_summary
            
        except Exception as e:
            print(f"  âš ï¸  íŠ¸ë Œë“œ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return {
                'keyword': industry_keyword,
                'current_trend': 'ë°ì´í„° ì—†ìŒ',
                'avg_interest': 0,
                'top_regions': [],
                'rising_queries': []
            }
    
    def load_documents(self):
        """ì—‘ì…€ íŒŒì¼ë“¤ì„ ì½ì–´ì„œ ë¬¸ì„œ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•"""
        print("\nğŸ“š ë¬¸ì„œ ë¡œë”© ì¤‘...")
        
        excel_files = list(self.data_folder.glob('*.xlsx'))
        
        for file_path in excel_files:
            try:
                # ì—‘ì…€ íŒŒì¼ ì½ê¸°
                df = pd.read_excel(file_path, sheet_name=0)
                
                # íŒŒì¼ëª…ì—ì„œ ì£¼ì œ ì¶”ì¶œ
                file_name = file_path.stem
                
                # ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                content = self._extract_content_from_df(df, file_name)
                
                self.documents.append({
                    'file_name': file_name,
                    'file_path': str(file_path),
                    'content': content,
                    'dataframe': df,
                    'raw_data': df.to_string()[:5000]  # ì²˜ìŒ 5000ìë§Œ ì €ì¥
                })
                
                print(f"  âœ“ {file_name}")
                
            except Exception as e:
                print(f"  âœ— {file_path.name}: {str(e)}")
        
        print(f"\nì´ {len(self.documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ!\n")
        
    def _extract_content_from_df(self, df, file_name):
        """ë°ì´í„°í”„ë ˆì„ì—ì„œ ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        content_parts = [file_name]
        
        # ì—´ ì´ë¦„ë“¤
        content_parts.append(" ".join([str(col) for col in df.columns if pd.notna(col)]))
        
        # ì²˜ìŒ 30í–‰ì˜ ë°ì´í„° ìƒ˜í”Œë§
        for idx, row in df.head(30).iterrows():
            row_text = " ".join([str(val) for val in row if pd.notna(val) and str(val).strip()])
            if row_text:
                content_parts.append(row_text)
        
        return " ".join(content_parts)
    
    def build_embeddings(self):
        """ë¬¸ì„œë“¤ì˜ ì„ë² ë”© ë²¡í„° ìƒì„±"""
        if len(self.documents) == 0:
            print("âŒ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤!")
            print("í•´ê²°ë°©ë²•:")
            print("  1. openpyxl ì„¤ì¹˜: pip install openpyxl")
            print("  2. ë°ì´í„° í´ë” ê²½ë¡œ í™•ì¸")
            return False
        
        print("ğŸ”„ ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì¤‘...")
        
        contents = [doc['content'] for doc in self.documents]
        self.doc_embeddings = self.model.encode(contents, show_progress_bar=True)
        
        print("âœ“ ì„ë² ë”© ìƒì„± ì™„ë£Œ!\n")
        return True
    
    def search_relevant_documents(self, store_data, top_k=2):
        """ë§¤ì¥ ë°ì´í„°ì™€ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰"""
        
        if len(self.documents) == 0:
            print("âŒ ê²€ìƒ‰í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤!")
            return []
        
        if self.doc_embeddings is None or len(self.doc_embeddings) == 0:
            print("âŒ ì„ë² ë”©ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
            return []
        
        # ë§¤ì¥ íŠ¹ì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        query_text = self._store_data_to_text(store_data)
        
        print("ğŸ” ë§¤ì¥ íŠ¹ì„± ë¶„ì„:")
        print(f"  {query_text[:200]}...\n")
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.model.encode([query_text])
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = cosine_similarity(query_embedding, self.doc_embeddings)[0]
        
        # ìƒìœ„ Kê°œ ë¬¸ì„œ ì¸ë±ìŠ¤
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        results = []
        print(f"ğŸ“Š ê´€ë ¨ë„ ë†’ì€ ìƒìœ„ {top_k}ê°œ ë¬¸ì„œ:\n")
        
        for rank, idx in enumerate(top_indices, 1):
            doc = self.documents[idx]
            similarity_score = similarities[idx] * 100
            
            results.append({
                'rank': rank,
                'file_name': doc['file_name'],
                'file_path': doc['file_path'],
                'similarity': similarity_score,
                'dataframe': doc['dataframe'],
                'raw_data': doc['raw_data']
            })
            
            print(f"  {rank}. [{similarity_score:.1f}%] {doc['file_name']}")
        
        print()
        return results
    
    def _store_data_to_text(self, store_data):
        """ë§¤ì¥ JSON ë°ì´í„°ë¥¼ ê²€ìƒ‰ìš© í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        
        parts = []
        
        # ê¸°ë³¸ ì •ë³´
        parts.append(f"ë§¤ì¥ëª…: {store_data.get('store_name', '')}")
        parts.append(f"ìƒê¶Œ: {store_data['analysis'].get('trade_area', '')}")
        
        # ê³ ê° íŠ¹ì„±
        analysis = store_data['analysis']
        parts.append(f"ê³ ê° í˜ë¥´ì†Œë‚˜: {analysis.get('persona', '')}")
        
        # ì£¼ìš” ì„¸ê·¸ë¨¼íŠ¸
        for seg in analysis.get('top_segments', []):
            parts.append(f"{seg['segment']} {seg['store_value']}")
        
        # ë°©ë¬¸ ìœ í˜•
        for visit in analysis.get('visit_mix', []):
            parts.append(f"{visit['factor']} {visit['store_value']}")
        
        # ì¶©ì„±ë„
        loyalty = analysis.get('loyalty', {})
        parts.append(loyalty.get('summary', ''))
        
        # ì¸ì‚¬ì´íŠ¸
        for insight in analysis.get('insights', []):
            parts.append(insight)
        
        # ì¶”ì²œì‚¬í•­
        for rec in store_data.get('recommendations', []):
            parts.append(rec)
        
        return " ".join(parts)
    
    def generate_marketing_strategy_with_gemini(self, store_data, relevant_docs, trend_data=None):
        """Gemini 2.5 Flashë¥¼ ì‚¬ìš©í•˜ì—¬ ë§ˆì¼€íŒ… ì „ëµ ìƒì„± (4ê°œ ì„¹ì…˜)"""
        
        print("=" * 80)
        print("ğŸ¤– Gemini 2.5 Flashë¡œ ë§ˆì¼€íŒ… ì „ëµ ìƒì„± ì¤‘...")
        print("=" * 80)
        print()
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self._build_gemini_prompt_v2(store_data, relevant_docs, trend_data)
        
        try:
            # Gemini API í˜¸ì¶œ
            response = self.gemini_model.generate_content(prompt)
            
            print("âœ… ì „ëµ ìƒì„± ì™„ë£Œ!\n")
            print("=" * 80)
            print("ğŸ¯ ë§ì¶¤í˜• ë§ˆì¼€íŒ… ì „ëµ")
            print("=" * 80)
            print()
            print(response.text)
            print()
            print("=" * 80)
            
            return response.text
            
        except Exception as e:
            print(f"âŒ Gemini API ì˜¤ë¥˜: {str(e)}")
            return None
    
    def _build_gemini_prompt_v2(self, store_data, relevant_docs, trend_data=None):
        """Geminiì—ê²Œ ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ ìƒì„± (ê°„ê²°í•œ 4ê°œ ì„¹ì…˜ ë²„ì „)"""
        
        analysis = store_data['analysis']
        
        # ë§¤ì¥ ì •ë³´ ìš”ì•½
        store_summary = f"""
## ğŸ“ ë§¤ì¥ ê¸°ë³¸ ì •ë³´
- **ë§¤ì¥ëª…**: {store_data['store_name']}
- **ìƒê¶Œ**: {analysis['trade_area']}
- **í´ëŸ¬ìŠ¤í„°**: {analysis['cluster']}

## ğŸ‘¥ í•µì‹¬ ê³ ê° íŠ¹ì„±
"""
        
        for seg in analysis['top_segments']:
            store_summary += f"- {seg['segment']}: {seg['store_value']} (í‰ê·  ëŒ€ë¹„ {seg['gap']})\n"
        
        store_summary += "\n## ğŸš¶ ë°©ë¬¸ ìœ í˜•\n"
        for visit in analysis['visit_mix']:
            store_summary += f"- {visit['factor']}: {visit['store_value']} (í‰ê·  ëŒ€ë¹„ {visit['gap']})\n"
        
        store_summary += f"\n## ğŸ’ ì¶©ì„±ë„\n{analysis['loyalty']['summary']}\n"
        
        # íŠ¸ë Œë“œ ì •ë³´ ì¶”ê°€
        trend_section = ""
        if trend_data:
            trend_section = f"""
## ğŸ“ˆ ì—…ì¢… íŠ¸ë Œë“œ (PyTrends)
- **í‚¤ì›Œë“œ**: {trend_data['keyword']}
- **í˜„ì¬ ì¶”ì„¸**: {trend_data['current_trend']}
- **í‰ê·  ê´€ì‹¬ë„**: {trend_data['avg_interest']}/100
- **ì¸ê¸° ì§€ì—­**: {', '.join(trend_data['top_regions'][:3])}
- **ê¸‰ìƒìŠ¹ ê²€ìƒ‰ì–´**: {', '.join(trend_data['rising_queries'][:3]) if trend_data['rising_queries'] else 'ì—†ìŒ'}
"""
        
        # ì°¸ê³  ë¬¸ì„œ ì •ë³´ (ê°„ê²°í•˜ê²Œ)
        docs_info = "\n## ğŸ“š ì°¸ê³  ë°ì´í„° ì†ŒìŠ¤\n\n"
        for doc in relevant_docs:
            docs_info += f"**[{doc['rank']}] {doc['file_name']}** (ìœ ì‚¬ë„ {doc['similarity']:.0f}%)\n"
            # ë°ì´í„° ì¼ë¶€ë§Œ í¬í•¨ (1500ìë¡œ ì œí•œ)
            docs_info += f"```\n{doc['raw_data'][:1500]}...\n```\n\n"
        
        # ìµœì¢… í”„ë¡¬í”„íŠ¸ (ê°„ê²°í•œ 4ê°œ ì„¹ì…˜)
        prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ ì™¸ì‹ì—… ë§ˆì¼€íŒ… ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ì‹¤í–‰ ê°€ëŠ¥í•˜ê³  ê°„ê²°í•œ** ë§ˆì¼€íŒ… ì „ëµì„ ì‘ì„±í•˜ì„¸ìš”.

{store_summary}

{trend_section}

{docs_info}

---

## ğŸ“ ìš”ì²­ì‚¬í•­

ë‹¤ìŒ **4ê°œ ì„¹ì…˜**ìœ¼ë¡œ ë§ˆì¼€íŒ… ì „ëµì„ ì‘ì„±í•˜ë˜, **ê° ì„¹ì…˜ì€ ê°„ê²°í•˜ê²Œ(3-5ê°œ bullet points)** ì‘ì„±í•˜ì„¸ìš”:

### ğŸ“Š ì„¹ì…˜ 1: RAG ë¬¸ì„œ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
- ì°¸ê³  ë¬¸ì„œì—ì„œ ë°œê²¬í•œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ 3ê°€ì§€
- í•´ë‹¹ ë§¤ì¥ì— ì ìš© ê°€ëŠ¥í•œ ì‹œì‚¬ì 
- **í˜•ì‹**: ë¬¸ì„œëª… ëª…ì‹œ + ì¸ì‚¬ì´íŠ¸ + ì ìš© ë°©ë²•

### ğŸ¯ ì„¹ì…˜ 2: ë§¤ì¥ íŠ¹ì„± ê¸°ë°˜ ì „ëµ
- ëª¨ë¸ë§ ë°ì´í„°(ê³ ê°ì¸µ, ë°©ë¬¸ìœ í˜•, ì¶©ì„±ë„)ë¥¼ í™œìš©í•œ ë§ì¶¤ ì „ëµ 3ê°€ì§€
- RAG ë¬¸ì„œì˜ ì¶”ê°€ ì¸ì‚¬ì´íŠ¸ ê²°í•©
- **í˜•ì‹**: ë°ì´í„° ê·¼ê±° + ì „ëµ + ì‹¤í–‰ ë°©ë²•

### ğŸ“± ì„¹ì…˜ 3: ìš°ì„ ìˆœìœ„ ì±„ë„ ì „ëµ
- ìƒìœ„ 3ê°œ ë§ˆì¼€íŒ… ì±„ë„ë§Œ ì„ ì • (ì˜ˆ: Instagram, ì¹´ì¹´ì˜¤í†¡, ë°°ë‹¬ì•±)
- ê° ì±„ë„ë³„ êµ¬ì²´ì  ì‹¤í–‰ ì•„ì´ë””ì–´ 2-3ê°œ
- **í˜•ì‹**: ì±„ë„ëª… + íƒ€ê²Ÿ + ì•¡ì…˜

### ğŸš€ ì„¹ì…˜ 4: ë§ˆì¼€íŒ… ì±„ë„ & í™ë³´ì•ˆ
- ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ êµ¬ì²´ì ì¸ í™ë³´ ìº í˜ì¸ 3-4ê°œ
- ê° ìº í˜ì¸ì˜ ì‹¤í–‰ ë°©ë²•ê³¼ ì˜ˆìƒ íš¨ê³¼ë¥¼ ëª…ì‹œ
- ë§¤ì¥ì˜ ê°•ì (ì¬ë°©ë¬¸ìœ¨, íŠ¹ì • ê³ ê°ì¸µ ë“±)ì„ í™œìš©í•œ ì°¨ë³„í™” ì „ëµ í¬í•¨

---

## âš ï¸ ì‘ì„± ê°€ì´ë“œë¼ì¸
1. **ê°„ê²°ì„±**: ê° ì„¹ì…˜ì€ ìŠ¤í¬ë¡¤ ì—†ì´ í•œ í™”ë©´ì— ë³´ì´ë„ë¡
2. **êµ¬ì²´ì„±**: "ì†Œì…œë¯¸ë””ì–´ í™œìš©" âŒ â†’ "ì¸ìŠ¤íƒ€ ë¦´ìŠ¤ 3ê°œ/ì£¼, í•´ì‹œíƒœê·¸ #ë‹µì‹­ë¦¬ì¹´í˜" âœ…
3. **ì‹¤í–‰ì„±**: ë‹´ë‹¹ìê°€ ë°”ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€
4. **ê·¼ê±° ì œì‹œ**: ê° ì „ëµë§ˆë‹¤ ë°ì´í„°/ë¬¸ì„œ ì¶œì²˜ ê°„ë‹¨íˆ ëª…ì‹œ
5. **ì´ëª¨ì§€ í™œìš©**: ê°€ë…ì„±ì„ ìœ„í•´ ì ì ˆíˆ ì‚¬ìš©

**ì´ ë¶„ëŸ‰**: A4 1ì¥ ì´ë‚´ (ì•½ 800-1200ì)
"""
        
        return prompt


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # 0. Gemini API í‚¤ ì„¤ì •
    GEMINI_API_KEY = "ì—¬ê¸°ì— ë‚´ í‚¤ê°’ ì…ë ¥"
    
    # API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ë‹¤ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸° ì‹œë„
    if GEMINI_API_KEY == "ì—¬ê¸°ì—_ë‹¹ì‹ ì˜_Gemini_API_í‚¤ë¥¼_ì…ë ¥í•˜ì„¸ìš”":
        GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
        if not GEMINI_API_KEY:
            print("âš ï¸  Gemini API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")
            print("ë°©ë²• 1: ì½”ë“œì—ì„œ GEMINI_API_KEY ë³€ìˆ˜ì— ì§ì ‘ ì…ë ¥")
            print("ë°©ë²• 2: í™˜ê²½ë³€ìˆ˜ GEMINI_API_KEY ì„¤ì •")
            print("\nAPI í‚¤ ë°œê¸‰: https://makersuite.google.com/app/apikey")
            return
    
    # 1. RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    data_folder = r"C:\Temp\data\ragë°ì´í„°_ë³‘ì§„ë‹˜"
    
    try:
        rag = RAGMarketingSystem(data_folder, GEMINI_API_KEY)
    except Exception as e:
        print(f"âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
        return
    
    # 2. ë¬¸ì„œ ë¡œë“œ ë° ì„ë² ë”© ìƒì„±
    try:
        rag.load_documents()
    except Exception as e:
        print(f"âŒ ë¬¸ì„œ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        return
    
    if len(rag.documents) == 0:
        print("\n" + "="*80)
        print("âš ï¸  ë¬¸ì„œë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        print("="*80)
        print("\ní•´ê²° ë°©ë²•:")
        print("1. openpyxl ì„¤ì¹˜:")
        print("   pip install openpyxl")
        print("\n2. PyTrends ì„¤ì¹˜:")
        print("   pip install pytrends")
        print("\n3. ê°€ìƒí™˜ê²½ì„ ì‚¬ìš© ì¤‘ì´ë¼ë©´ ê°€ìƒí™˜ê²½ì— ì„¤ì¹˜:")
        print("   (venv) pip install openpyxl pytrends")
        print("\n4. ë°ì´í„° í´ë” ê²½ë¡œ í™•ì¸:")
        print(f"   í˜„ì¬ ê²½ë¡œ: {data_folder}")
        print(f"   í´ë” ì¡´ì¬ ì—¬ë¶€: {Path(data_folder).exists()}")
        return
    
    if not rag.build_embeddings():
        return
    
    # 3. ë§¤ì¥ ë°ì´í„°
    store_data = {
        "store_code": "003AC99735",
        "store_name": "ë©”ê°€ì»¤í”¼ ë‹µì‹­ë¦¬ì ",
        "status": "ì¬ë°©ë¬¸ ê³ ê°ì¸µì´ ë§¤ìš° íƒ„íƒ„í•´ìš”",
        "industry_keyword": "ì¹´í˜",
        "analysis": {
            "summary": "í•µì‹¬ ê³ ê°ì€ 10-20ëŒ€ ë‚¨ì„±ì…ë‹ˆë‹¤",
            "persona": "í•µì‹¬ ê³ ê°ì€ 10-20ëŒ€ ë‚¨ì„±ì…ë‹ˆë‹¤ (í´ëŸ¬ìŠ¤í„° ëŒ€ë¹„ +6.11pp)",
            "cluster": 3,
            "top_segments": [
                {"segment": "10-20ëŒ€ ë‚¨ì„±", "store_value": "14.22%", "gap": "+6.11pp"},
                {"segment": "30ëŒ€ ë‚¨ì„±", "store_value": "17.40%", "gap": "+3.52pp"}
            ],
            "visit_mix": [
                {"factor": "ìœ ë™ ê³ ê° ë¹„ì¤‘", "store_value": "53.05%", "gap": "+8.22pp"},
                {"factor": "ì§ì¥ ê³ ê° ë¹„ì¤‘", "store_value": "9.81%", "gap": "-5.52pp"},
                {"factor": "ì£¼ê±° ê³ ê° ë¹„ì¤‘", "store_value": "37.14%", "gap": "-2.70pp"}
            ],
            "loyalty": {
                "summary": "ì¬ë°©ë¬¸ ê³ ê° ë¹„ì¤‘ì´ í´ëŸ¬ìŠ¤í„° í‰ê· ë³´ë‹¤ +4.55pp ë†’ìŠµë‹ˆë‹¤",
                "metrics": [
                    {"metric": "ì¬ë°©ë¬¸ ê³ ê° ë¹„ìœ¨", "store_value": "36.99%", "gap": "+4.55pp"},
                    {"metric": "ì‹ ê·œ ê³ ê° ë¹„ìœ¨", "store_value": "4.70%", "gap": "-1.48pp"},
                    {"metric": "ì¶©ì„± ê³ ê° ì§€ìˆ˜", "store_value": "32.29ì ", "gap": "+6.18ì "}
                ]
            },
            "insights": [
                "10-20ëŒ€ ë‚¨ì„± ë¹„ì¤‘ì´ í´ëŸ¬ìŠ¤í„° í‰ê·  ëŒ€ë¹„ +6.11ppì…ë‹ˆë‹¤.",
                "ìœ ë™ ê³ ê° ë¹„ì¤‘ì´ í´ëŸ¬ìŠ¤í„° í‰ê· ë³´ë‹¤ +8.22ppë¡œ ë†’ìŠµë‹ˆë‹¤.",
                "ì¬ë°©ë¬¸ ê³ ê° ë¹„ìœ¨ì´ í´ëŸ¬ìŠ¤í„° í‰ê· ë³´ë‹¤ +4.55ppë¡œ ë†’ìŠµë‹ˆë‹¤."
            ],
            "trade_area": "ë‹µì‹­ë¦¬"
        },
        "recommendations": [
            "10-20ëŒ€ ë‚¨ì„± ê³ ê° ìœ ì…ì„ ëŠ˜ë¦¬ì„¸ìš”.",
            "ìœ ë™ ê³ ê°ì„ ìœ„í•œ ë¹ ë¥¸ í”½ì—…ì„ ê°•ì¡°í•˜ì„¸ìš”.",
            "ì¶©ì„± ê³ ê°ì„ ìœ„í•œ í”„ë¦¬ë¯¸ì—„ ë©¤ë²„ì‹­ì„ í™•ì¥í•˜ì„¸ìš”."
        ]
    }
    
    # 4. PyTrendsë¡œ ì—…ì¢… íŠ¸ë Œë“œ ë¶„ì„
    try:
        trend_data = rag.get_industry_trends(store_data.get('industry_keyword', 'ì¹´í˜'))
    except Exception as e:
        print(f"âš ï¸  íŠ¸ë Œë“œ ë¶„ì„ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {str(e)}")
        trend_data = None
    
    # 5. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    try:
        relevant_docs = rag.search_relevant_documents(store_data, top_k=2)
    except Exception as e:
        print(f"âŒ ë¬¸ì„œ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return
    
    if len(relevant_docs) == 0:
        print("âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 6. Geminië¡œ ë§ˆì¼€íŒ… ì „ëµ ìƒì„± (íŠ¸ë Œë“œ ë°ì´í„° í¬í•¨)
    try:
        strategy = rag.generate_marketing_strategy_with_gemini(store_data, relevant_docs, trend_data)
    except Exception as e:
        print(f"âŒ ì „ëµ ìƒì„± ì˜¤ë¥˜: {str(e)}")
        return
    
    # 7. ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (ì˜µì…˜)
    if strategy:
        try:
            output_file = "marketing_strategy_output.txt"
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write("=" * 80 + "\n")
                f.write("ğŸ¯ ë§ì¶¤í˜• ë§ˆì¼€íŒ… ì „ëµ\n")
                f.write("=" * 80 + "\n\n")
                f.write(f"ë§¤ì¥: {store_data['store_name']}\n")
                f.write(f"ìƒì„±ì¼: {pd.Timestamp.now()}\n\n")
                
                # íŠ¸ë Œë“œ ì •ë³´
                if trend_data:
                    f.write("ğŸ“ˆ ì—…ì¢… íŠ¸ë Œë“œ:\n")
                    f.write(f"  - í‚¤ì›Œë“œ: {trend_data['keyword']}\n")
                    f.write(f"  - ì¶”ì„¸: {trend_data['current_trend']}\n")
                    f.write(f"  - ê´€ì‹¬ë„: {trend_data['avg_interest']}/100\n\n")
                
                # ì°¸ê³  ë¬¸ì„œ
                f.write("ì°¸ê³  ë¬¸ì„œ:\n")
                for doc in relevant_docs:
                    f.write(f"  - [{doc['similarity']:.1f}%] {doc['file_name']}\n")
                f.write("\n" + "=" * 80 + "\n\n")
                f.write(strategy)
            
            print(f"\nğŸ“„ ê²°ê³¼ê°€ '{output_file}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸  íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {str(e)}")


if __name__ == "__main__":
    main()